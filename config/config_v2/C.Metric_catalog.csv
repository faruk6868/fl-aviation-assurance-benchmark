Metric ID,Metric Name,Metric Definition,Justification,Related Requirement ID(s)
M.CERT.MAINT_RDY,Certification Maintenance Readiness (pass/fail),"Availability of secure data download, retest procedures, and drift workflows to retain certification credit.",Supports sustained airworthiness evidence and efficient re-approval processes.,R.L.3.8
M.CERT.PHASE_RDY,Phased Approval Readiness (pass/fail),"Evidence package readiness for preliminary assessment, compliance demonstration, and final approval gates.",Aligns certification planning with the authority�s phased approval framework.,R.V.2.2
M.CFG.CONF_CMPL,Configuration Management Conformance (%),Percentage of ML artifacts under version control with reproducible build and change records.,"Maintains integrity and reproducibility across datasets, models, and code throughout the lifecycle.",R.L.1.1
M.CHANGE.CC_INDEX,Change Control Compliance Index,"Composite index of adherence to training, aggregation, validation, deployment approval gates and PCCP criteria.",Controls post-approval evolution of models with pre-defined guardrails and regression checks.,R.L.2.2; R.L.3.7
M.CLS.Pre,Precision (Global),Precision = TP / (TP + FP) computed on independent test set; report macro/weighted averages.,Complements safety-causal metrics by quantifying false alarm propensity on unbiased evaluation.,R.M.1.1
M.CLS.F1_Sc,F1-Score (Global),F1 = 2 � (Precision � Recall) / (Precision + Recall) on independent test set.,Balances precision and recall to provide a single discriminative performance figure for maintenance classification.,R.M.1.1
M.CLS.AUC,AUC (ROC),Area under ROC curve computed by trapezoidal integration across thresholds on independent test set.,Threshold-free separability indicator to support independent test evaluation of trained and inference artifacts.,R.M.1.3; R.M.1.4
M.COTS.DEACT_COV,COTS Function Deactivation Coverage (%),Percentage of unused or out-of-scope COTS functions identified and deactivated or justified with no safety impact.,Reduces unintended capability exposure and attack surface from unused functions.,R.L.4.2
M.DATA.ODD_PARAM,ODD Parameterization Completeness (%),"Share of ODD parameters that are explicitly specified, versioned, and approved out of the authoritative ODD parameter list.",Confirms the system operates only within a well-defined and version-controlled ODD as required for safe deployment.,R.D.1.1
M.DATA.ODD_COV,ODD Coverage Ratio (%),Portion of ODD operating envelopes and edge cases represented in the dataset weighted by risk and usage frequency.,Verifies that development and test data sufficiently and representatively cover the defined ODD including hazard-driven edge cases.,R.D.1.2
M.DATA.DQR_CONF,Data Quality Conformance Rate (%),"Percentage of Data Quality Requirements (relevance, origin, format, accuracy, integrity, completeness, representativeness) satisfied by audited datasets.",Assures datasets meet documented DQRs to reduce development risk and support credible verification outcomes.,R.D.2.1; R.D.2.2
M.DATA.VERIF_COV,Data Verification Coverage (%),Percentage of ODD-appropriateness and adequacy checks executed and passed prior to training.,Provides objective evidence that datasets are adequate for the intended ODD prior to model training.,R.D.2.3
M.DATA.SPLIT_INT,Dataset Separation Integrity (pass/fail + score),"Composite result of leakage tests across splits (hash overlap, near-duplicate detection, label leakage probes) with auditable logs.",Prevents cross-contamination between train/validation/test/certification sets to maintain unbiased evaluation.,R.D.2.4
M.DATA.REQ_VAL,Requirements Validation Completion (%),Share of captured requirements validated and signed-off prior to development start.,Ensures requirement completeness and feasibility are confirmed before downstream activities proceed.,R.D.2.5
M.DATA.PREPROC_AUDIT,"Data Pre-Processing Audit Rate	","Proportion of data samples with documented and reproducible pre-processing and feature engineering history.	","Ensures that all data pre-processing and feature engineering operations are transparent, reproducible, and auditable, in line with data governance and regulatory expectations for safety-critical ML systems.",R.D.2.6
M.DRIFT.DETECT.MTTD,Drift Detection Mean Time to Detection,Average time (in batches or hours) from drift onset to detector alarm in the operational monitoring timeline.,Ensures monitoring reacts rapidly enough to preserve safety margins during post-deployment shifts.,R.L.3.6
M.DRIFT.DETECT.RECALL,Drift Detection Recall,Proportion of drift-affected windows correctly identified by the detector.,Shows the monitoring stack is sufficiently sensitive to distributional drift to avoid missed degradations.,R.L.3.6
M.DRIFT.DETECT.FAR,Drift Detection False Alarm Rate,Proportion of nominal windows where the detector raises an alarm despite no drift.,Constrains nuisance alarms that could desensitize operators and violate lifecycle monitoring guidance.,R.L.3.6
M.DRIFT.KS,KS Drift Statistic,Two-sample Kolmogorov�Smirnov statistic on error/input distributions with rolling-window testing.,Provides statistical power view complementary to MTTD/recall for drift readiness.,R.L.3.6
M.FL.NONIID,Client Non-IID Index (0�1) and Registry Completeness (%),Quantifies heterogeneity across clients (quantity/label/feature/temporal drift) and completeness of the client cohort registry.,"Characterizes federated data diversity to inform aggregation, evaluation sampling, and fairness monitoring.",R.D.6.1
"M.FL.COMM_BYTES	",Communication Efficiency (bytes/round),Average payload per training round and total budget within operational bandwidth/latency constraints.,Verifies feasibility of FL in constrained aviation environments.,R.F.1.1
M.FL.CONV_TIME,Time-to-Convergence (rounds/min),Number of rounds and wall-clock time to achieve acceptance-level performance under representative conditions.,Confirms FL training completes within operational timelines and resource limits.,R.F.1.2
"	M.FL.PRIV_DP","Privacy Budget (epsilon, delta)",Differential privacy parameters and resulting utility deltas for the trained model.,Balances privacy guarantees with performance thresholds in safety-critical contexts.,R.F.2.1
M.FL.PRIV_EPS,"Privacy Budget (epsilon)","Achieved epsilon for the selected DP variant, representing the privacy loss bound for the completed training run.",Provides a scalar privacy guarantee suitable for thresholding and comparison across DP variants.,R.F.2.1
M.FL.PRIV_DELTA,"Privacy Budget (delta)","Achieved delta for the selected DP variant, representing the probability of privacy failure associated with epsilon.",Ensures delta stays within the allowable order-of-magnitude for aviation maintenance deployments.,R.F.2.1
M.FL.ATTACK_RES,Attack Resistance (success rate %),"Observed success rates of inference, gradient leakage, poisoning, and backdoor attacks under the defined threat model.",Quantifies resilience of FL pipeline against salient threat classes.,R.F.3.1
M.FL.BYZ_TOL,Robust Aggregation Tolerance (%),Maximum fraction of Byzantine/adversarial clients tolerated without violating acceptance targets.,Assures robust aggregation meets threat model requirements.,R.F.3.2
"M.FL.WORST_CLIENT	",Worst-Group Performance (min),Minimum performance across clients/groups and gap to overall performance.,Prevents harm to disadvantaged clients and enforces fairness constraints.,R.F.4.1
M.FL.BENEFIT_EQ,Benefit Equity Index,Equity of performance gains across clients over baseline and monitoring of disparities.,Ensures FL benefits are distributed equitably across the fleet.,R.F.4.2
"M.FL.COMM_COMP	",Communication Efficiency Ratio,Compression ratio = uncompressed payload / compressed payload averaged per round.,Quantifies payload reduction benefits of compression/sparsification for bandwidth-constrained FL.,R.F.1.1
"M.FL.ATTACK_DET	",Attack Detection Rate,Detected malicious updates / total injected malicious updates under defined threat models.,Quantifies effectiveness of attack detection required by robust aggregation/defense strategies.,R.F.3.2
M.GEN.GAP,Generalization Gap,Absolute difference between train and test performance for primary metric on matched splits.,Directly quantifies overfitting consistent with generalization bound expectations.,R.M.1.5
M.HF.SA_INDEX,Situation Awareness Index (individual/shared),Composite SAGAT-like score for individual and shared situation awareness in simulated operational tasks.,Verifies the system supports maintaining accurate understanding of state and context by all operators.,R.H.1.1; R.H.1.2
 M.HF.TRAIN_DESKILL,Training Completion & De-Skilling Risk,Completion rate of required training modules and residual de-skilling risk level after mitigation.,Confirms user competencies and mitigations are in place against automation overreliance.,R.H.2.1; R.H.2.2
M.LA.PROC_CMPL,Learning Assurance Process Completeness (%),"Percentage of prescribed learning assurance elements documented with roles, activities, and checkpoints.",Makes AI/ML-specific verification activities explicit and reviewable.,R.L.2.1
M.LM.SPEC_CMPL,Learning Management Specification Completeness (%),Percentage of required learning management and training process requirements captured with thresholds.,Ensures robust management of the learning process with measurable criteria.,R.M.2.1
M.LOCK.POST_DEP,Post-Deployment Learning Disabled (pass/fail),Telemetric and configuration evidence that online learning is disabled in operation unless explicitly approved.,Prevents unapproved drift of model behavior during operations to maintain certified state.,R.L.2.3
M.MON.RUNTIME_COV,Runtime Monitoring Coverage Index,"Coverage of ODD bounds monitoring, output confidence monitoring, and metric stability alarms with escalation procedures.",Detects departures from design assumptions in real time and escalates appropriately.,R.L.3.3; R.L.3.4; R.L.3.5
M.OP.OP_USAGE,Overarching Properties Utilization (pass/fail),Documented use of Overarching Properties as alternative means when traditional objectives are insufficient.,Establishes acceptable compliance paths for AI/ML characteristics.,R.V.2.4
M.OPS.SAFETY_DATA,Operational Safety Data Program Coverage (%),Availability and completeness of operational data recording plus continuous safety assessment cadence adherence.,Enables continuous safety assurance with sufficient operational evidence and defined triggers.,R.L.3.1; R.L.3.2
M.PERF.COV,Independent Test Evaluation Coverage (%),Percentage of required evaluations completed and documented for trained and inference artifacts on independent test sets.,Demonstrates objective performance on unbiased datasets for both training and deployed artifacts.,R.M.1.3; R.M.1.4
M.PERF.STAT_REP,Generalization Reporting Completeness (%),Presence of generalization bounds with two-sided 95% confidence intervals for primary metrics.,Supports out-of-sample claims with statistically sound uncertainty reporting.,"R.M.1.5; R.M.1.6, R.M.4.2"
M.PERF.CROSS_STAB,Cross-Population Stability (worst-group ?),Maximum absolute gap between overall performance and worst client/group performance across key metrics.,Ensures safety and effectiveness for all intended users and clients without harmful disparities.,R.M.1.7
M.PRG.PHM,Prognostics Score (PHM08),PHM08 score using asymmetric penalties for early/late RUL predictions; lower is better.,Adds domain-specific early/late penalty structure for RUL beyond RMSE in prognostics tasks.,R.M.1.1
M.QA.AUDIT_PASS ,QA Audit Pass Rate (%),Proportion of lifecycle process audits passed without major findings within the audit period.,Provides objective oversight of process adherence and maturity.,R.L.1.2
M.REG.LEV_TRACE,Regulatory Leverage Traceability (pass/fail),Trace matrix showing which existing regulations and performance-based requirements are used and where deviations occur.,Provides transparency on how existing rules are leveraged and where alternative means are applied.,R.V.2.3
M.REUSE.IMPACT_CMPL,Model Reuse Impact Assessment Completeness (%),"Presence and completeness of documented assumptions, limitations, and re-verification for reused or transferred models.",Assures reuse does not invalidate safety and performance claims in the target context.,R.L.4.1
 M.ROB.STAB_ODD,Stability Across ODD (CV %),Coefficient of variation of key metrics across ODD segments and transitions under repeated testing.,Confirms stable performance across validated operating regions and transitions.,"R.M.2.2, R.M.4.1, R.M.5.2"
M.ROB.DEGRAD,Robustness Degradation Index (%),Relative drop in key metrics under defined adverse training and inference conditions versus nominal.,Quantifies resilience to perturbations and distribution shifts in both training and deployment phases.,R.M.2.3; R.M.2.4
M.ROB.STRESS_COV ,Stress Test Coverage (%),Percentage of planned item-to-aircraft interface stress tests executed and passed.,Detects anomalous behaviors under extreme but plausible conditions to harden the system.,R.M.2.5
M.RT.LAT,Inference Latency (ms),Average per-sample latency with batch=1 on representative hardware; report percentiles.,Demonstrates real-time feasibility within operational processing constraints.,R.F.1.2
 M.SAF.ACC,Safety-Linked Accuracy (min),Minimum accuracy threshold derived from FHA allocation mapped per-cycle,Ensures overall error rate respects the allocated safety budget at the evaluation timebase to satisfy safety-linked performance requirements.,R.M.1.1; R.M.1.2
M.SAF.REC,Safety-Linked Recall (min),Minimum recall threshold derived from FHA allocation per-cycle using the same allocation as accuracy when recall is safety-causal.,Prioritizes missed-event risk (FNs) per hazard analysis by enforcing a minimum detection rate consistent with allocated safety risk.,R.M.1.1; R.M.1.2
M.SAF.FPFN,Safety-Linked FPR/FNR (max),Proportion of evaluated models or configurations that simultaneously satisfy the hazard-analysis-derived maximum false negative rate (FNR_max) and maximum false positive rate (FPR_max),Directly bounds harmful decision error rates using allocated probability budgets to maintain aircraft-level safety objectives.,R.M.1.1; R.M.1.2
M.SAF.FNR_MAX,Safety-Linked FNR (max),False negative rate for the safety classifier evaluated against the FHA-derived maximum.,Provides explicit gating for missed detections independent of false alarm behaviour.,R.M.1.1; R.M.1.2
M.SAF.FPR_MAX,Safety-Linked FPR (max),False positive rate for the safety classifier evaluated against the FHA-derived ceiling.,Limits nuisance alerts that could desensitise maintenance personnel.,R.M.1.1; R.M.1.2
M.SAF.RMSE,Safety-Linked RMSE (max),Maximum RMSE for RUL or timing estimates set by the safety buffer L and normal tail quantile z: RMSE_max = L/z ,Links regression dispersion to buffer overrun risk per cycle so exceedances do not surpass the allocated safety probability.,R.M.1.1; R.M.1.2
M.SAFE.LINK_TRACE,Safety Linkage Traceability (pass/fail),Binary indicator with evidence that metric thresholds are traced to safety objectives and hazard analysis.,Enforces explicit linkage from safety analysis to acceptance criteria for auditability.,R.M.1.1
 M.SAFE.HAZ_PRIOR,Hazard-Driven Prioritization Factor,Ratio of FN to FP stringency in acceptance limits as justified by hazard analysis.,Implements stricter limits on the more safety-critical error mode per FHA.,R.M.1.2
M.SEC.RISK_COV,Security Risk Coverage (%),Coverage of identified AI/ML-specific threats with mapped mitigations validated for effectiveness.,"Confirms risks like poisoning, extraction, and adversarial perturbations are mitigated and re-tested.",R.D.5.1; R.D.5.2
M.STAT.CI,95% CI Width,Width of two-sided 95% confidence interval for primary metric via bootstrap or analytic variance.,"Measures statistical uncertainty magnitude, complementing CI presence checks.",R.M.1.6
M.STD.STD_COMP,Consensus Standards Compliance (%),Percentage of applicable clauses from referenced consensus standards implemented with evidence.,Demonstrates alignment with SAE/EU standards to support certification credibility.,R.V.2.1
M.SYS.FUNC_TRACE,Functional Decomposition Traceability (%),Proportion of decomposed functions correctly allocated with trace links to implementation artifacts acknowledging ML-specific breaks in hierarchy.,Maintains clear allocation and acknowledges ML uniqueness in architecture traceability.,R.D.3.1
M.SYS.AILVL_SET,AI Involvement Level Determination (pass/fail),Binary indicator with evidence that AI Involvement Level was defined and approved with rationale.,Determines assurance rigor proportional to AI�s role in the function and associated risks.,R.D.3.2
M.TRACE.BIDIR,Traceability Completeness (%),"Share of requirements with bidirectional links to data, model, V&V cases, and results.",Supports certification review with end-to-end trace links and evidence mapping.,R.L.1.3
M.TRUST.ETHICS_IDX,Trustworthiness & Ethics Compliance Index,"Checklist score across safety, lawfulness, robustness, ethics, bias controls, and data protection compliance.",Operationalizes high-level trustworthiness principles and privacy obligations into verifiable checkpoints.,R.D.4.1; R.D.4.2; R.D.4.3
M.VV.OBJ_COV,V&V Objective Coverage (%),"Coverage of explainability, predictability, auditability, and traceability objectives with executed tests and evidence.",Assures V&V scope addresses the authority�s objectives for AI/ML systems.,R.V.1.1
 M.XAI.USE_SCORE,Explanation Usability Score,"Task-based human factors score for clarity and relevance of explanations of model scope, behavior, outputs, and uncertainty.",Ensures explanations are understandable by intended operators to support safe decisions.,R.E.1.1
M.XAI.FIDE_STAB,Attribution Fidelity & Stability,Quantitative fidelity w.r.t. model gradients/probes and run-to-run stability measured on held-out slices.,Validates that feature attributions reflect the model and are consistent across runs and seeds.,R.E.1.2
M.XAI.FIDELITY,Attribution Fidelity Score,Average alignment of attribution vectors with gradient-based references across sampled perturbations.,Ensures explanations faithfully reflect model sensitivities.,R.E.1.2
M.XAI.STABILITY,Attribution Stability Score,Minimum/robust statistic of attribution similarity across repeated noisy evaluations.,Demonstrates explanations remain consistent under small perturbations or re-runs.,R.E.1.2
 M.XAI.DOC_CMPL,Explanation Documentation Completeness (%),"Presence of method specs, configurations, data scope, and known limitations in explainability documentation.",Provides reviewable evidence for explainability approach scope and limits.,R.E.1.3
M.XAI.CLARITY_RATE,Presentation Clarity Pass Rate (%),Share of explanatory outputs rated clear and unambiguous by end users in representative tasks.,Confirms explanations are communicated at the right abstraction level for the task and user.,R.E.1.4
M.XAI.VALID_COV,Explanation Validity Coverage (%),Fraction of relevant outputs for which the specified explanation type has been validity-checked with evidence.,Ensures each explanation type is valid for its intended outputs before use in operations.,R.E.1.5
M.ARCH.DOC_SCORE ,Architecture Documentation Score,Presence and completeness of architecture documentation for all model constituents.,"Requires manual review of system documentation and architecture artifacts; cannot be computed from model or dataset alone.	",R.D.3.3
M.FLOW.TRACE_IDX,"Flowdown Traceability Index	","Fraction of derived requirements mapped and evidenced to subsystem processes.	","Project and process management metric; requires audit of requirements tracking tools or documentation, not possible via dataset or FL benchmarking.	",R.D.3.4
M.BIAS.COMPL,"Model Bias/Variance Compliance Rate	","Share of models satisfying pre-defined thresholds for bias and variance.	","Bias and variance can be calculated and thresholded for all FL models trained/running on C-MAPSS instances; completely automatable.	",R.M.3.2
M.BIAS.TRADEOFF ,Bias�Variance Trade-off Profile,"Aggregated characterization of bias and variance across candidate model families, reported as a Pareto-style bias�variance profile or frontier, used to justify the selected model family.",This metric explicitly captures how the bias�variance trade-off was explored across model families and provides quantitative evidence that the selected family represents an acceptable balance for the safety and performance objectives,R.M.3.1.
M.XFORM.DELTA,"Model Transformation Impact Delta	","Performance gap between pre- and post-transformation model on held-out test data.	","Directly measurable after each conversion, optimization or retraining step using C-MAPSS test splits and metrics; part of standard ML/FL auditing.	",R.M.5.1
M.INFER.STAB,"Inference Stability Score	","Coefficient of variation of inference performance across repeated test runs and ODD segments.	","Running repeated inference on dataset with FL models allows direct computation of this metric; used to detect drift or instability quickly.	",R.M.5.2
M.IMP.SAFE_IMPACT,Derived Requirement Safety Impact Coverage,Proportion of derived requirements that have an explicitly documented safety impact assessment and updated links to the safety assessment and affected subsystem requirements.,"This metric directly measures whether every derived requirement has been analyzed for its impact on safety and subsystem requirements, going beyond mere validation completeness",R.D.3.5
