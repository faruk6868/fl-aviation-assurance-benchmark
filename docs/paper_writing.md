A Regulatory-Aligned Assurance Framework for Federated Learning in Aviation with a Prognostics and Diagnostics Testbed
Abstract
1.	Introduction
[Message: Starting from general problem pdm in aviation and its consequences. Then approaches nowadays adopted. What is the missing points of todays innovations and what should be integrated why.]
Aviation field is as important as other critical fields as any missing alert or warning can turn into a disaster.  It is crucial to collect these data in advance and process it in a way that any dire consequences can be 	avoided. Aviation would be the best example of data collection in other words data silos and not benefitting it properly for the sake of maintenance [reference]. These data silos can be used in fleet intelligence integrating different aircraft fleets, airlines and stakeholders. Yet, centralised Machine Learning models which is highly cited in academic research is not capable of converting any degradation data into a fleet wide learning due to numerous reasons such as privacy concerns, centralised nature, robustness etc [reference].
[message: usage of FL in addressing this problem and issues in regulatory end to integrate] 
It is obvious that decentralised or collaborative learning methods have potential to address these issues. Federated learning (FL) has emerged as a decentralised learning paradigm without requiring raw data aggregation, protecting privacy while enabiling robust and collaborative learning [reference]. These improvements are promising for privacy, robustness and fleet-enabled learning limitations in aviation. Fedavg is one of the earliest examples from google as a basemodel [reference]. Despite its clear technological and strategic advantages, the adoption of FL in safety-critical aviation system is impeded by a significant regulatory and validation gap. A systematic review of the guidance from major aviation regulatories, including the European Union Aviation Safety Agency (EASA) and the U.S. Federal Aviation Administration (FAA), regarding the federated learning deployment indicates evident absence of regulations and standards or any pathway for any type of distributed learning. While these authorities have established their AI roadmaps, these frameworks addresses general ML principles and does not consider the distributed learning architecture, security, and data governance challenges specific to decentralised learning like FL. Current EASA guidelines [6], for instance, explicitly states that “online or adaptive” learning which FL by nature within that group is not covered by the existing standards. This uncertainty is not affecting the industry, but also the academic improvements are not systematically addressed or benefit due to lack of regulatory standards or guidelines which is essential for airworthiness. 
[msg: so what if there is no reg or standards how it is related to the our idea]
It appears that this regulatory void is a formidable obstacle. Without sound evidence based regulatory-deriven assurance frameworks for evaluating FL systems in aviation maintenance applications development, airworthiness claims, and deployment of it will be far from reality, despite growing evidence of FL’s technical maturity. Bridging this gap requires more than technical maturity. It requires systematic pathway derived from regulatory requirements to evaluate all aspects of FL such as data governance, performance, robustness, explainability in a way that auditors and regulators can trust and act upon.
[msg: how we did]
Motivated by these challenges and sector priorities, this work proposes a regulatory aligned assurance framework tailored for benchmarking fleet-wide FL for aviation prognostics and diagnostics. The core logic of our approach is not to invent a standard from scratch, but rather to adapt and synthesize proven safety principles that regulatory authorities in aviation and other safety-critical domains already trust and utilize. This is achieved through a hierarchical synthesis of established concepts, integrating assurance pyramid depicted in Figure 2 (i) the high-level "trustworthiness" principles from the EASA and FAA AI roadmaps, (ii) the process-oriented requirements of emerging standards for AI in aviation, such as SAE ARP6983, and (iii) mature best practices for data-driven systems from adjacent safety-critical domains, including automotive (ISO 26262, ISO/PAS 21448) and Food and Drug Administration (FDA). We begin by defining 85 normative clauses categorised under 6 different regulatory concepts, including FAA, EASA, FDA, STD, FL, XAI. Then, related requirements and metrics are derived from these regulatory clauses, spanning seven themes: Data Governance, Model Performance and Robustness, Lifecycle and Process assurance, Explainability, Human Factors, Verification and validation, Federated learning specific. In the benchmarking framework, each requirement is linked to specific measurement metrics, which are divided into applicable and non-applicable categories, for a total of 76 metrics.
[additional details]
Using Functional Hazard Assessment (FHA) methods, we derive safety-critical thresholds for key metrics including accuracy, False Negative/Positive and other critical metrics, while other thresholds are derived from literature, industrial and regulatory sources. We then design a 14-experiment federated learning pipeline evaluating these metrics across varied scenarios including centralized baseline, IID/non-IID data, communication efficiency, Byzantine resilience, differential privacy, concept drift, fairness, explainability.
To facilitate traceability and assurance argumentation, we construct a Concentric Assurance Map (CAM) that relates regulations to requirements, requirements to metrics, and metrics to experimental evidence. Our empirical validation uses this pipeline to benchmark three mostly cited FL algorithms (FedAvg, FedProx, SCAFFOLD) on the C-MAPSS turbofan dataset as a use case.
This study addresses two primary research questions and makes a fourfold contribution:
RQ1: In the absence of formal regulatory standards, how can Federated Learning (FL) assurance for safety-critical aviation prognostics and diagnostics be systematically, quantitatively, and reproducibly assessed using requirements and metrics explicitly derived from existing authority guidance and operational hazard analysis?
RQ2: How does adopting such an assurance framework reveal and help quantify the trade-offs, limitations, and comparative advantages of leading FL strategies in the presence of non-IID data, communication challenges, security threats, and real-world operational constraints?
Our contributions are: 
(i) Framework Contribution: We propose, for the first time, a regulatory-derived assurance benchmarking framework for Federated Learning in aviation. The framework is meticulously synthesized from foundational safety principles and explicit regulatory requirements including EASA's AI Roadmap, FAA's AI Safety Assurance initiatives, SAE and ISO standards, and best practices from medical and automotive safety. It translates these into 76 concrete requirements, and operationalizes them using 38 traceable metrics, with thresholds systematically set.
(ii) Pipeline and Metric Engineering Contribution: We develop and release an auditable, multi-experiment FL assurance benchmarking framework wherein each of the 38 assurance metrics is measured and compared against defined thresholds across a diverse set of 14 evaluation scenarios (including communication, resilience, privacy, fairness, explainability, non-IID, and real-time operational constraints). Excluded metrics also serve as an additional guidance for anyone interested in documentation and process checks.  The design enables direct, transparent linkage between regulatory requirements, metrics, and experimental evidence via a Concentric Assurance Map (CAM).
(iii) Empirical and Comparative Contribution: Using this framework and pipeline, we conduct the first systematic, multi-metric empirical evaluation of three leading FL algorithms (FedAvg, FedProx, SCAFFOLD) on the C-MAPSS turbofan prognostics and diagnostics tasks, rigorously quantifying their assurance benchmark not only predictive performance, but governance, fairness, robustness, and explainability. We report actionable analytics on strengths, weaknesses, and trade-offs, directly informing both practitioners and standards development.
(iv) Community and Roadmapping Contribution: The entire framework, metric suite, and results are made publicly available to enable reproducibility, traceable assurance reporting, and further benchmarking by academics and practitioners supporting the incremental pathway to regulatory acceptance and providing a foundation for anticipated EASA/FAA guidance on FL assurance in future AI regulations.
 
Figure 1 Assurance pyramid
2.	Related Work
Related work in this study is organised around three interconnected strands: (i) predictive maintenance and emerging applications of federated learning in aviation and related industries, (ii) regulatory and assurance initiatives for AI in aviation, and (iii) assurance frameworks from other safety‑critical domains and existing AI/FL benchmarking approaches. Together, these strands highlight a clear misalignment between technically mature FL methods for predictive maintenance and the current state of aviation‑grade assurance and certification practices.
Predictive maintenance and federated learning
Predictive maintenance and PHM have been extensively studied in aviation, with a strong focus on turbofan engines and C‑MAPSS‑derived datasets, where centralised machine learning and deep learning models dominate the literature for RUL prediction and fault diagnosis (X. Li et al., 2018; Ramasso & Saxena, 2014). While these works demonstrate high technical performance, they typically assume that heterogeneous data from multiple aircraft or fleets can be aggregated into a single repository, thereby sidelining real‑world constraints such as organisational data silos, privacy, and cross‑stakeholder data governance (Dalgkitsis et al., 2024).
In parallel, federated learning has gained traction as a privacy‑preserving paradigm for predictive maintenance in industrial environments, including manufacturing, process industries, and automotive systems, (Jenkel et al., 2023; Z. Li et al., 2022; Zhang et al., 2021). Studies in these sectors show that FL can enable collaborative model training across distributed assets or plants without raw data sharing, while handling non‑IID data distributions and communication constraints to some extent, (McMahan, Moore, et al., 2017). A small but growing body of work applies FL to prognostics and remaining useful life estimation, including engine and component health monitoring, indicating potential benefits for aviation maintenance scenarios,(Chen et al., 2023; Landau et al., 2026; Rosero et al., 2020). Yet these contributions largely assess FL using technical metrics such as accuracy, communication cost and resilience, without embedding them into a broader safety and assurance context tailored to airworthiness needs (Landau et al., 2026; Rosero et al., 2020).
AI regulation and assurance in aviation
Aviation safety regulation has historically relied on rigorous, process‑oriented standards such as DO‑178C (Paul et al., 2023) and related guidance, which assume static, deterministic software artefacts and do not directly accommodate data‑driven, adaptive or distributed learning systems (FAA, 2022). In response to the rise of AI, EASA has articulated an AI Roadmap and associated concept papers that introduce high‑level trustworthiness principles, including safety, explainability, human oversight and lifecycle assurance for machine learning applications, and propose a phased approach to integrating AI into certification frameworks, (EASA, 2023, 2024). However, these documents explicitly note that “online” or “adaptive” learning is outside the scope of existing standards, leaving paradigms such as federated and other distributed learning architectures without a clearly defined regulatory pathway, (EASA, 2024).
On the U.S. side, recent FAA initiatives outline an AI safety assurance vision for aviation and recognise the distinct challenges posed by ML systems compared with traditional avionics software (FAA, 2022), but they similarly stop short of providing detailed requirements for federated, cross‑organisational or continuously updated models in safety‑critical maintenance applications (FAA, 2022). Concurrently, emerging standards such as SAE ARP6983 and its associated EUROCAE documents (EDA, 2025) aim to define recommended practices for AI in aeronautical systems, with an emphasis on process requirements, data management and verification for ML components. Nonetheless, initial scopes focus mainly on offline, supervised models embedded into conventional development lifecycles, and do not yet address the specific assurance, data governance and security issues introduced by FL‑style decentralised architectures (EDA, 2025). This leaves aviation stakeholders without a structured, regulation‑derived framework for arguing the safety and airworthiness of FL‑enabled prognostics and diagnostics despite growing technical feasibility (Dalgkitsis et al., 2024).
Assurance frameworks and AI/FL benchmarking
Other safety‑critical domains offer more mature patterns for structuring assurance of data‑driven and ML‑based systems, which can be instructive for aviation FL (EDA, 2025). In automotive, ISO 26262 establishes a comprehensive functional safety lifecycle, and ISO/PAS 21448 (SOTIF) explicitly targets hazards arising from limitations of perception, data distributions and intended functionality, thereby addressing issues that are highly relevant for ML‑centric perception and decision systems (ISO, 2022). In the medical domain, the FDA’s AI/ML‑based SaMD initiatives introduce a “total product lifecycle” perspective for adaptive algorithms, emphasising good machine learning practices, pre‑specified change control plans and systematic post‑deployment performance monitoring as core elements of assurance (FDA, 2025, 2025). Across these sectors, structured assurance cases and traceability from regulatory principles to concrete requirements and evidence have emerged as key mechanisms to make ML systems auditable and acceptable to regulators (EASA, 2023).
At the same time, there is a growing body of generic AI assurance work and FL benchmarking suites that evaluate algorithms like FedAvg, FedProx and SCAFFOLD under varied non‑IID, adversarial, communication‑constrained and privacy‑sensitive scenarios (Chai et al., 2022; Karimireddy et al., 2020). (Hu et al., 2022) present a study aimed at creating OARF, a benchmark suite for Federated Learning (FL) systems. This work rigorously emphasizes experimental design by providing generalized methods and metrics to quantitatively assess complex FL properties, such as model accuracy, communication cost, and various privacy mechanisms. However, OARF was lacking in regulatory alignment for safety-critical applications, as its focus remained purely on technical performance and scalability across generalized data silos, without addressing domain-specific standards or certification needs.
In a similar vein, Chai et al. proposed FedEval, a holistic evaluation framework built on the FedEval-Core taxonomy, meticulously covering four essential technical aspects: Privacy, Robustness, Effectiveness, and Efficiency. This research excels in methodological rigor, urging for comprehensive evaluation procedures to expose trade-offs between goals like efficiency and utility. Despite this comprehensive technical evaluation, FedEval stops short of providing assurance necessary for aviation, focusing solely on compatibility across general machine learning benchmarks, including comparing FL accuracy to centralized accuracy while omitting the need to integrate quantifiable results with aviation safety assurance criteria, such as EASA's quantifiable generalisation guarantees (LM-04) (EASA, 2024).
These domain-agnostic benchmarks, while advancing the technical understanding of FL performance, fail to provide the traceable mapping required by aviation stakeholders to demonstrate airworthiness and compliance. This gap motivates the present work, which seeks to synthesise trusted safety principles from aviation and adjacent domains into a regulatory‑aligned assurance framework and to operationalise it as a multi‑experiment FL benchmarking pipeline specifically tailored to aviation maintenance.
3. Methodological Framework
3. Methodology
This section describes the methodological structure used to construct, operationalise, and validate the proposed regulatory‑aligned assurance framework for federated learning in aviation prognostics and diagnostics. The approach consists of four main stages: (i) regulatory‑driven clause synthesis, (ii) requirement and metric engineering, (iii) threshold derivation, and (iv) design of a federated learning assurance testbed as described in Figure 2. The outputs of these stages are then integrated into a Concentric Assurance Map (CAM), which serves as a traceability and assurance tool linking regulatory clauses, requirements, metrics, and experimental evidence.
 
Figure 2 FL Assurance Benchmarking framework
3.1 Regulatory-driven clause synthesis
The methodology begins with a structured synthesis of normative guidance from aviation and adjacent safety‑critical domains. Building on the analysis in Section 2, regulatory and standardisation documents from EASA, FAA, SAE/EUROCAE, ISO and FDA are systematically reviewed and decomposed into atomic, verifiable clauses relevant to data‑driven and AI‑enabled systems. Each statement that expresses an obligation, constraint, or expectation regarding safety, data, AI behaviour, lifecycle processes, or human oversight is rephrased into a normative clause.
In this step, 85 clauses are derived and grouped into six regulatory concept categories: FAA‑specific clauses, EASA‑specific clauses, FDA‑related clauses, generic safety and standards (STD), federated learning (FL‑specific) and explainable AI (XAI). This grouping enables both vertical (authority‑specific) and horizontal (theme‑specific) analysis. To avoid inventing new standards, only concepts and obligations that can be clearly traced back to existing regulatory or de facto authoritative sources are retained, and ambiguous or purely aspirational statements are excluded. The resulting clause set forms the top layer of the assurance “pyramid” that the rest of the methodology operationalises.
3.2 Requirement, metric and threshold engineering
In the second stage, each normative clause is translated into one or more assessable requirements, and each requirement is then linked to concrete metrics wherever possible. This step ensures that high‑level regulatory expectations can be evaluated quantitatively in the context of a federated learning system.
First, the resulting canon comprises 74 requirements grouped into seven domains: Data & Governance, Model Performance & Robustness, Lifecycle & Process Assurance, Explainability, Human Factors, Verification & Validation, and Federated Learning Specific
For each theme, requirements are written in a structured form (theme, sub section of the theme, requirement ID, requirement statement, and linked normative clauses) in a machine-readable way for the reproducibility purpose and annotated with the source that justify them. Requirements that can be directly tested in an experimental FL setup are then associated with measurable metrics, resulting in a total of 76 metrics. Metrics are classified as “applicable” or “non‑applicable” depending on whether they can be operationalised with observable data in the experimental setup, or whether they pertain to process and documentation aspects outside the scope of the runtime pipeline, for instance, configuration management, training records, organisational roles. These are not just simply excluded metrics rather these are guidelines for regulators, domain experts and academics. Each metric recorded with their metric ID, definition, justification and related traceable requirements. As highlighted in the Sankey figure. 
Eligible 38 applicable metrics’ threshold values are then derived using a combination of standards, industrial source, and ranges reported in the literature for comparable tasks.  For threshold levels, it must be admitted that it is highly dependent on criticality of component and numerous conditions. We applied systematically in order to show how to use this framework. 

3.5 Federated learning assurance testbed
The final stage operationalises the requirements–metric structure into a reproducible FL assurance testbed that generates auditable evidence for the Concentric Assurance Map. The testbed is instantiated on the C‑MAPSS turbofan dataset (default FD002 for mixed operating conditions) and distributes engines across approximately 20 simulated clients to induce realistic non‑IID behaviour through label skew and feature shifts. Three reference FL strategies—FedAvg, FedProx, and SCAFFOLD—are trained under aligned hyper‑parameters, while centralised baselines are retained for equity and benefit analyses. All experiments are orchestrated by a single CLI (`run_benchmark.py`) that fixes random seeds, logs configuration artefacts, and writes metric outputs in a standard schema (`tbXX_results_{algo}.csv`) enabling end‑to‑end traceability from configuration to evidence.

Scenario coverage is organised into 14 test beds (TB‑01…TB‑14) that each target distinct assurance themes and map directly to specific requirement families. Data governance and operating domain validity are checked in TB‑01 (non‑IID quantification, ODD coverage) and TB‑02 (statistical generalisation and uncertainty). Safety‑critical behaviour is assessed through classification of failure horizons (TB‑03) and RUL regression (TB‑04) with hazard‑aligned thresholds. Population‑level equity and stability are exercised in TB‑05 and TB‑06, probing worst‑group performance and cross‑population benefit of FL compared with centralised training. Robustness across operational drifts and perturbations is stressed in TB‑07 (ODD boundary shifts) and TB‑08 (concept drift detection power). Communication and efficiency properties are captured by TB‑09 (compression/quantisation budgets) and TB‑10 (convergence profiles). Trustworthiness extensions are provided by TB‑11 (differential privacy budget vs utility), TB‑12 (Byzantine/adversarial resilience), TB‑13 (runtime footprint for edge deployment), and TB‑14 (attribution fidelity and stability for explainability).
Across all test beds, metric computation is bound to predefined min/max thresholds from the requirement catalogue; critical metrics trigger pass/fail outcomes, while non‑critical metrics are reported for monitoring. Outputs are consolidated into `all_results.csv` and visual artefacts (heatmaps, Sankey flows, CAM links) to support structured assurance arguments. This design allows regulators, auditors, and practitioners to reproduce the experiments, trace each metric to its originating requirement and clause, and compare FL algorithms under a consistent, regulation‑aligned evidence protocol.
