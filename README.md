# A Regulatory-Aligned Assurance Framework for Federated Learning in Aviation with a Prognostics and Diagnostics Testbed

**Abstract (10 lines)**
1) Benchmarks federated learning (FL) models for aviation prognostics using NASA C-MAPSS turbofan data. 2) Aligns assurance evidence with aviation guidance (EASA/FAA), SAE ARP4754A/4761, SOTIF, and NIST AI RMF. 3) Implements 14 test beds (TB-01 to TB-14) spanning data realism, safety, robustness, fairness, privacy, runtime, and explainability. 4) Evaluates 38 applicable metrics with thresholds mapped to requirements and clauses. 5) Automates Claim-Argument-Evidence (CAM) and Goal Structuring Notation (GSN) asset generation. 6) Provides deterministic seeding, config snapshots, and reproducible aggregation. 7) Uses only public data (NASA C-MAPSS) with helper scripts to download and stage files. 8) Reproduces pass/fail tables and paper figures (heatmap, runtime) via one command. 9) Separates code, configs, results, and paper assets for auditability. 10) Ships software citation metadata and guidance to archive releases on Zenodo.

## Quickstart
1) Python 3.10 recommended. Create env and install deps:
```
python -m venv .venv
.venv\Scripts\activate   # or: source .venv/bin/activate
pip install -r requirements.txt
```
2) Download C-MAPSS (public) into `data/c-mapss/`:
```
python scripts/download_cmapss.py --source-url <public_zip_url> --target-dir data/c-mapss
```
3) Run the full benchmark (uses provided measurement JSONs):
```
python -m src.benchmark.run --config configs/default.yaml
```
Outputs land in `results/testbeds/`, `results/assurance_reports/`, `results/all_results.csv`, and figures are copied to `paper_assets/figures/`.

## Reproduce paper results
- One-shot reproduction (requires Bash such as Git Bash/WSL/macOS/Linux):
```
bash scripts/run_all.sh
```
This runs TB01-TB14 with `fedavg`, aggregates to `results/all_results.csv`, and stages heatmap/runtime plots into `paper_assets/figures/`.

- Figures reproduced: assurance heatmap, runtime line/stacked plots (from `results/time_benchmarks/`).
- Illustrative assets: CAM/GSN diagrams in `artifacts/cam` and `artifacts/gsn` (kept as generated artifacts, not regenerated by default).

## Repository map
- `src/benchmark/`: entrypoint (`run.py`) with config loading, seeding, orchestration, aggregation, figure copying.
- `configs/`: `default.yaml` plus `tb01.yaml` ... `tb14.yaml` placeholders for per-test-bed overrides.
- `src/analysis/`: aggregation utilities used by scripts and the benchmark runner.
- `src/federated/`, `src/models/`, `src/metrics/`, `src/testbeds/`, `src/evaluation/`: existing FL algorithms, models, metric evaluators, TB loaders, and reporting pipeline.
- `analysis/`: plotting scripts for metric/radar/Sankey/heatmap/time benchmarks (produce `results/figures/` and `results/time_benchmarks/`).
- `artifacts/`: measurement JSONs per TB/algorithm plus CAM/GSN artifacts.
- `results/`: runtime outputs (not versioned; placeholders provided).
- `paper_assets/`: collected figures/tables for the paper.
- `docs/`: plain-language assurance framework, CAM overview, reproducibility guidance.
- `scripts/`: `run_all.sh`, `download_cmapss.py`, `generate_paper_assets.py` helpers.
- `environment/`: notes for Conda/Docker users; `environment.yml` for Conda.

## Audience guidance
- Regulators/auditors: run `bash scripts/run_all.sh`, review `results/assurance_reports/` and `results/all_results.csv`, and trace clause→requirement→metric links via `docs/assurance_framework.md` and config snapshots in `results/run_configs/`.
- Industry practitioners: use `configs/default.yaml` as a baseline, swap measurement JSONs in `artifacts/testbeds/`, and tune thresholds/requirements in `config/config_v2/` as your operational ODD changes.
- Researchers: add algorithms under `src/federated/`, metrics under `src/metrics/`, and TB overrides in `configs/tb*.yaml`; regenerate figures via `analysis/` scripts and stage them with `scripts/generate_paper_assets.py`.

## How to cite
- See `CITATION.cff` for software metadata.
- For archival DOI, create a GitHub release, enable the Zenodo GitHub integration, and let Zenodo mint a DOI. Update the DOI in `CITATION.cff` and cite as:
  - Authors. (Year). *A Regulatory-Aligned Assurance Framework for Federated Learning in Aviation with a Prognostics and Diagnostics Testbed* (Version X.Y.Z) [Computer software]. DOI: <zenodo-doi>

## License
- Licensed under Apache-2.0 to encourage broad reuse while preserving notice and disclaimer obligations. See `LICENSE`.

## Governance / safety disclaimer
- Research prototype; not certified for operational aviation use.
- Results depend on thresholds, assumptions, and provided measurement artifacts.
- Users are responsible for validating applicability to their operational context.
